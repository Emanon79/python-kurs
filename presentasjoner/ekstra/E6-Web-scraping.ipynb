{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7df72244-f3af-4216-b857-b6778624ced9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# E6 - Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7b84e3-3336-4e9d-87c4-f0f289d70b4b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## E6.1 Hvorfor web scraping?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9829c51c-0ceb-4515-b571-f2d093682394",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Det er mange nettsider som ikke gir tilgang til sine data gjennom APIer. Hvis man alikevel ønsker å laste ned dataene for videre prosessering kan man benytte seg av web scraping. For å gjøre web scraping må man skrive kode for nedlastning og parsing av websider og tilhørende ressurser. I python kan man gjøre dette med bibliotekene requests og BeautifulSoup4. Biblioteket requests tar seg av nedlastning, og BeautifulSoup4 tar seg av parsingen av html. Noen ressurser, som feks script, krever at man skriver regulære-uttrykk for å hente ut nødvendig informasjon.\n",
    "\n",
    "Det må sies at web scraping bør gjøres på en god og hensiktsmessig måte. Du har selv et ansvar for å begrense last på nettsiden og respektere opprettshav for informasjonen du laster ned. De fleste nettsider i dag har en robots.txt hostet under root-folderen. Man bør ta en titt på denne filen før man scraper en nettside."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ac9592-2757-492d-a48b-b97da237914f",
   "metadata": {},
   "source": [
    "## E6.2 Verktøy\n",
    "Når man skal scrape en nettside bør man starte med å gjøre seg kjent med nettsidens struktur og ressurser. Dette kan enkelt gjøres i nettleseren sin utvikler-verktøy. I browserens utvikler-verktøy kan man utforske en nettsides struktur ved å søke og se på nettverkstrafikk samt hvilke filer som er lastet ned.\n",
    "\n",
    "I firefox og chrome får man tilgang til utvikler-verktøy ved å trykke F12. 'Inspector' gir mulighet for å utforske html. 'Network' gir tilgang til nettverkstrafikk. Av og til må man laste siden på nytt for å få opp trafikken. Under 'Storage' kan man se hvilke ressurser som er lastet ned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6875e732-e52a-4ea8-af50-6c8333596c58",
   "metadata": {},
   "source": [
    "## E6.3 Et enkelt eksempel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98e88972-60bb-4c93-a1b0-1cfd5787ffed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Andorra', 'Andorra la Vella', '84000', '468.0'], ['United Arab Emirates', 'Abu Dhabi', '4975593', '82880.0'], ['Afghanistan', 'Kabul', '29121286', '647500.0'], ['Antigua and Barbuda', \"St. John's\", '86754', '443.0'], ['Anguilla', 'The Valley', '13254', '102.0'], ['Albania', 'Tirana', '2986952', '28748.0'], ['Armenia', 'Yerevan', '2968000', '29800.0'], ['Angola', 'Luanda', '13068161', '1246700.0'], ['Antarctica', 'None', '0', '1.4E7'], ['Argentina', 'Buenos Aires', '41343201', '2766890.0']]\n"
     ]
    }
   ],
   "source": [
    "import requests               # https://requests.readthedocs.io/en/latest/\n",
    "from bs4 import BeautifulSoup # https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "\n",
    "def download_webpage(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    return response.text\n",
    "\n",
    "def extract_data(webpage):\n",
    "    soup = BeautifulSoup(webpage)\n",
    "    # print(page.prettify())\n",
    "\n",
    "    data = []\n",
    "    \n",
    "    # Legg merke til bruk av class_ istedenfor class. Konvensjonell nødvendighet.\n",
    "    for match in soup.find_all(class_=\"col-md-4 country\"):\n",
    "        country_name = list(match.find(class_=\"country-name\").children)[2].string.strip()\n",
    "        country_capital = match.find(class_=\"country-capital\").string\n",
    "        country_population = match.find(class_=\"country-population\").string\n",
    "        country_area = match.find(class_=\"country-area\").string\n",
    "        \n",
    "        data.append([country_name, country_capital, country_population, country_area])\n",
    "        \n",
    "    return data\n",
    "\n",
    "url = \"http://www.scrapethissite.com/pages/simple/\"\n",
    "data = extract_data(download_webpage(url))\n",
    "\n",
    "print(data[:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13a7ffb-8598-47f9-9fee-401b09c4fcc0",
   "metadata": {},
   "source": [
    "## E6.4 Undersøk robots.txt for nyttig informasjon og begrensninger\n",
    "[Google create robot.txt](https://developers.google.com/search/docs/crawling-indexing/robots/create-robots-txt)\n",
    "[Brightdata](https://brightdata.com/blog/how-tos/robots-txt-for-web-scraping-guide)\n",
    "[Zenrows](https://www.zenrows.com/blog/robots-txt-web-scraping)\n",
    "\n",
    "Det er ikke alltid at det som står i robots.txt stemmer. Feks kan noen nettsider indikere at alle agenter får lov til å scrape og samtidig blokkere agenter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e1f758-7658-4a75-b9ad-5074ac98e7e2",
   "metadata": {},
   "source": [
    "### Noen nettsider krever at du later som at du er en nettleser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8422c263-0dd1-4d27-80e8-29fbd1520905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function download_webpage at 0x74c3505e2fc0>: FAILED\n",
      "<function download_webpage_browser at 0x74c3505e2de0>: SUCCESS\n"
     ]
    }
   ],
   "source": [
    "import requests               # https://requests.readthedocs.io/en/latest/\n",
    "from bs4 import BeautifulSoup # https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "import time\n",
    "\n",
    "def download_webpage(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    return response.text\n",
    "\n",
    "def download_webpage_browser(url):\n",
    "    response = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:131.0) Gecko/20100101 Firefox/131.0\"})\n",
    "    response.raise_for_status()\n",
    "    return response.text\n",
    "\n",
    "url = \"https://www.brewersfriend.com/homebrew-recipes/\"\n",
    "\n",
    "for func in [download_webpage, download_webpage_browser]:\n",
    "    try:\n",
    "        webpage = func(url)\n",
    "        print(f\"{func}: SUCCESS\", flush=True)\n",
    "    except:\n",
    "        print(f\"{func}: FAILED\", flush=True)\n",
    "\n",
    "    # https://www.brewersfriend.com/robots.txt\n",
    "    # Crawl-Delay: 20\n",
    "    if func != download_webpage_browser:\n",
    "        time.sleep(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fe853f-39af-405b-9112-32be4a33f032",
   "metadata": {},
   "source": [
    "### Behov for cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baca9f9-ab6b-4449-bc23-ea93062e8a42",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Når en nettside har en stor begrensning på hvor ofte du kan laste ned nye sider eller ressurser så kan det lønnes seg å cache data i lokalt filsystem. Data som ligger lokalt trenger man ikke å begrense tilgang til. Her finnes det biblioteker man kan benytte seg av."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09c0609-4a55-45dc-8140-1235ad9864cc",
   "metadata": {},
   "source": [
    "# E6.5 Oppgaver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782aff1a-3357-4175-bb9e-2fdf93d57931",
   "metadata": {},
   "source": [
    "Hvis du ikke har nett tilgjengelig så ligger det eksempel-sider under files/scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35279ad5-e615-4ddf-baed-015adf29e9bb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "1. Velg en av nettsidene under og lag kode som laster ned og henter ut relevant data fra hovedsiden\n",
    "  - http://books.toscrape.com/\n",
    "  - http://quotes.toscrape.com/\n",
    "2. Implementer rate-limiting med 1 request per 5 sekunder i funksjonen for nedlastning.\n",
    "3. Implementer en caching-mekanisme som lagrer nettsider som er lastet ned i funksjonen for nedlastning.\n",
    "   Du trenger en fil med en \"url\":\"data/filenavn\" mapping og evt. en mappe med data.\n",
    "   Biblioteket json er nyttig for å lagre og laste mapping-filen.\n",
    "4. Utvid oppgave 1 ved å hente ut mer informasjon fra linker assosiert med dataene fra hovedsiden.\n",
    "   Se etter href-tagen i under-elementer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
